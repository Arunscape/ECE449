We are looking at linear models. Specifically, the perceptron.
In exercise 2, we also look at the MLP, or multilayer perceptron.

Differentiability is important, so that we can find a direction to minimize error
in the error space. When the activation function is differentiable, we can then
go in the direction which is the negative gradient in the error space to
minimize error. This allows us to backpropagate the model's error when training
so that the weights can be optimized.

A linear activation function is usually a poor choice, since the output would
then be just a linear transformation of the input. In other words, it would just
as easily be represented as a matrix multiplication, so the outputs would 
generally not be very interesting. 

