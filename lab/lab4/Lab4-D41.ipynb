{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ECE 449 - Intelligent Systems Engineering<br><br>\n",
    "Lab 4: Neural Networks - Self-Organizing Maps</center></h1>\n",
    "<hr>\n",
    "<b>Lab date:</b> <i>Thursday, October 31, 2019 -- 2:00 - 4:50 PM</i>\n",
    "<br>\n",
    "<b>Room:</b> <i>ETLC E5-013</i>\n",
    "<br>\n",
    "<b>Lab report due:</b> <i>Wednesday, November 13, 2019 -- 3:50 PM</i>\n",
    "<hr>\n",
    "\n",
    "<h2>1. Objectives</h2>\n",
    "The objective of this lab is to learn the concepts behind self-organizing maps, or Kohonen networks, and apply them to real-world datasets to examine how effective they can be.\n",
    "\n",
    "<h2>2. Expectations</h2>\n",
    "Complete the pre-lab, and hand it in before the lab starts.  A formal lab report is required for this lab, which will be the completed version of this notebook.  There is a marking guide at the end of the lab manual.  If figures are required, label the axes and provide a legend when appropriate.\n",
    "\n",
    "<h2>3. Pre-lab</h2>\n",
    "1. What is the curse of dimensionality? Why is dimensionality reduction even necessary?\n",
    "\n",
    "<h2>4. Introduction</h2>\n",
    "<i>Self-organizing maps</i>, or <i>Kohonen networks</i>, are neural networks that employ <i>unsupervised learning</i>. This means, that no targets are presented to the network for training.  They map high dimensional input vectors onto a map with a lower dimension (often one- or two-dimensional, in this lab two-dimensional), and are commonly used for clustering.\n",
    "<br>\n",
    "In order to realize such a network, <i>competitive learning</i> is used.  This involves an input vector activating a neuron with a weight vector closest to the input vector, determined by Euclidean distance:\n",
    "<br><br>\n",
    "$$\n",
    "tot_i = \\sqrt{\\sum_{j=0}^{n} (w_j - x_j)^2}\n",
    "$$\n",
    "<br>\n",
    "The winning neuron's weights are updated to values that are closer to the input vector, according to the following formula:\n",
    "<br><br>\n",
    "$$\n",
    "\\Delta w_i = \\alpha (x_i-w_i)\n",
    "$$\n",
    "<br>\n",
    "where $\\alpha$ is the learning rate.  Additionally, neurons that are close to the winning neuron are updated as well, according to a neighborhood function.  Various functions can be used, but in general, the further the neuron is from the winning neuron, the smaller its weights shift towards to the input vector.\n",
    "<br>\n",
    "This process is repeated for each input vector for a large number of iterations, resulting in a map that clusters data in a way that preserves the topology of the inputs, and therefore revealing similarities in the inputs when visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. Experimental Procedure / Assignment </h2>\n",
    "While doing research or working as an engineer it is often important to get an intuitive grasp on the data that you are working with. With multidimensional data, this is often more complicated than \"just plotting everything in one figure\" because of the amount of dimensions and our lack of ability to intuitively grasp 4+ dimensional relations.  \n",
    "This lab will confront you with 2 multidimensional datasets and require you to develop intuitive understanding for what is happening in those datasets. As a tool for better understanding, we will need some sort of dimensionality reduction - which will be <i>Self-Organizing maps</i>.\n",
    "\n",
    "<b>The two datasets and exercises you will work with:</b>  \n",
    "The first batch of questions (<b>Exercise 1</b>) revolve around the Iris dataset again. The Iris Dataset is a set basically every machine learning student works with at some point, usually in the early beginning and revolving around a classification task. This will be the case here. Your overall task will be to use a SOM on this dataset to cluster and to classify, outperforming an alternative classification tool. <b>Exercise 1</b> is worth 65% of the overall assignment and pretty straightforward.  \n",
    "In <b>Exercise 2</b> you will work on a more obscure and mixed dataset. You will have more freedom and this exercise set will require a little more creativity and critical thinking. The overarching task here will be pattern recognition and interpretation, while the questions are more open-ended and require more fundamental understanding than in <b>Exercise 1</b>.  \n",
    "<b>It is recommended to start with Exercise 1.</b>\n",
    "\n",
    "<b>Methodology:</b> \n",
    "The lab will walk you through the fetching of the datasets and supply the basic functions needed for completing each task.  \n",
    "The idea behind this is to minimize the amount of coding and programming, and encourage experimentation from your side. Each supplied code snipped will be adequately documented, so you can modify it if you feel like it (just please make a safety copy of the function first...).\n",
    "\n",
    "\n",
    "\n",
    "Run the cell below to import the libraries required to complete this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'for all the Calculations and import stuff'\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio             # Loads .mat variables\n",
    "\n",
    "'To plot beautiful figures'\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pylab import plot,axis,show,pcolor,colorbar,bone\n",
    "\n",
    "'The Ai-modules of this lab'\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing  # Data preprocessing\n",
    "from minisom import MiniSom        # SOM toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 1:&nbsp;&nbsp; Self-organizing map on the IRIS Dataset</h3>\n",
    "<br>\n",
    "&emsp;<b>Task 1.1:</b>    \n",
    "During this exercise you will train a SOM to cluster the dataset. As a benchmark against classification, your SOM will compete with a simple decision tree.\n",
    "The exercise provides you with most of the functions that you will need. The goal is to develop a baseline understanding what SOMs are good at, and what they might have difficulties with.  \n",
    "But let's start slower and at the beginning: Run the following cell to import the dataset and its description. To make sure you can properly interpret the results, you can spend the next couple of minutes reading about plants, or continue on and return when you need to give an explanation for your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Use Scikit-learn to import the Iris Dataset, print a little literature on the set'\n",
    "from sklearn import datasets\n",
    "IRIS = datasets.load_iris()\n",
    "print(IRIS.DESCR)\n",
    "\n",
    "'What is a petal, what is a sepal?! ... Versicolor/Setosa/Virginica what?! ... What is an Iris flower???? '\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Task 1.2:</b>  \n",
    "Run the next cells to provide the 'naive' plots of the dataset's dimensional relations (created by projecting the data onto a two-dimensional plane):\n",
    "- SepalLength  \n",
    "- SepalWidth  \n",
    "- PetalLength  \n",
    "- PetalWidth  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'This cell provides some plotting. It also splits the data into SepalLength, SepalWidth, PetalLength, PetalWidth'\n",
    "\n",
    "'The following separation is basically for you :-) '\n",
    "SepalLength = IRIS.data[:,0]\n",
    "SepalWidth = IRIS.data[:,1]\n",
    "PetalLength = IRIS.data[:,2]\n",
    "PetalWidth = IRIS.data[:,3]\n",
    "y = IRIS.target # 0 == Iris Setosa, 1 == Iris Versicolor, 2 == Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'All them easy plots'\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(SepalLength[y==2], SepalWidth[y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(SepalLength[y==1], SepalWidth[y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(SepalLength[y==0], SepalWidth[y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(SepalLength[y==2], PetalLength[y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(SepalLength[y==1], PetalLength[y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(SepalLength[y==0], PetalLength[y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Petal length')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(SepalLength[y==2], PetalWidth[y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(SepalLength[y==1], PetalWidth[y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(SepalLength[y==0], PetalWidth[y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.legend()\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.plot(PetalLength[y==2], PetalWidth[y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(PetalLength[y==1], PetalWidth[y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(PetalLength[y==0], PetalWidth[y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.plot(PetalLength[y==2], SepalWidth[y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(PetalLength[y==1], SepalWidth[y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(PetalLength[y==0], SepalWidth[y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.plot(PetalWidth[y==2], SepalWidth[y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(PetalWidth[y==1], SepalWidth[y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(PetalWidth[y==0], SepalWidth[y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel('Petal width')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Task 1.3:</b>  \n",
    "Provide some comments below on the plots and the information you can extract from them. Do they provide you with an intuitive understanding of the dataset's interactions?  \n",
    "Please also give a short explanation of how using a SOM could help us to represent this dataset in a more digestible way. What are the parameters we need to take care of, how does a SOM work in general, and how would you see it working here?\n",
    "If you already have an idea about which parameter should be in which range, go ahead, drop the number, giving your justification."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Task 1.4:</b>  \n",
    "The next cell provides a basic function *Train_and_Plot([X,Y], InData, steps)* to train a SOM of dimensionality X times Y to represent the input data after a number of training steps. After execution, it will plot the SOM, comparing it to the Iris labels. The SOM classifies each cluster as the dominant class in the cluster, yielding a classification accuracy.  \n",
    "Run the cell to compile the function and the followup cell to call the function for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def plot_legend():\n",
    "    ''' Plots a legend for the colour scheme\n",
    "    given by abc_to_rgb. Includes some code adapted\n",
    "    from http://stackoverflow.com/a/6076050/637562'''\n",
    "\n",
    "    # Basis vectors for triangle\n",
    "    basis = np.array([[0.0, 1.0], [-1.5/np.sqrt(3), -0.5],[1.5/np.sqrt(3), -0.5]])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111,aspect='equal')\n",
    "\n",
    "    # Plot points\n",
    "    a, b, c = np.mgrid[0.0:1.0:50j, 0.0:1.0:50j, 0.0:1.0:50j]\n",
    "    a, b, c = a.flatten(), b.flatten(), c.flatten()\n",
    "\n",
    "    abc = np.dstack((a,b,c))[0]\n",
    "    #abc = filter(lambda x: x[0]+x[1]+x[2]==1, abc) # remove points outside triangle\n",
    "    abc = map(lambda x: x/sum(x), abc) # or just make sure points lie inside triangle ...\n",
    "\n",
    "    data = np.dot(abc, basis)\n",
    "    colours = [abc_to_rgb(A=point[0],B=point[1],C=point[2]) for point in abc]\n",
    "\n",
    "    ax.scatter(data[:,0], data[:,1],marker=',',edgecolors='none',facecolors=colours)\n",
    "\n",
    "    # Plot triangle\n",
    "    ax.plot([basis[_,0] for _ in range(3) + [0,]],[basis[_,1] for _ in range(3) + [0,]],**{'color':'black','linewidth':3})\n",
    "\n",
    "    # Plot labels at vertices\n",
    "    offset = 0.25\n",
    "    fontsize = 32\n",
    "    ax.text(basis[0,0]*(1+offset), basis[0,1]*(1+offset), '$A$', horizontalalignment='center',\n",
    "            verticalalignment='center', fontsize=fontsize)\n",
    "    ax.text(basis[1,0]*(1+offset), basis[1,1]*(1+offset), '$B$', horizontalalignment='center',\n",
    "            verticalalignment='center', fontsize=fontsize)\n",
    "    ax.text(basis[2,0]*(1+offset), basis[2,1]*(1+offset), '$C$', horizontalalignment='center',\n",
    "            verticalalignment='center', fontsize=fontsize)    \n",
    "\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def Train_and_Plot_SOM(dimensions, inputData, steps):\n",
    "    dimensions = np.array(dimensions)\n",
    "    inputData = np.transpose(inputData)\n",
    "    # Create SOM\n",
    "    som = MiniSom(\n",
    "        int(dimensions[0]),                                # Number of neurons (x-axis)\n",
    "        int(dimensions[1]),                                # Number of neurons (y-axis)\n",
    "        np.shape(inputData)[1],             # Number of elements in an input vector\n",
    "        sigma = .9,                        # Spread of the neighbourhood function\n",
    "        learning_rate = 0.5,                # Initial learning rate\n",
    "        neighborhood_function = 'gaussian'  # Type of neighborhood function\n",
    "    )\n",
    "\n",
    "    som.random_weights_init(inputData)  # Initialize weights of neurons randomly\n",
    "    som.train_random(inputData, steps)  # Train the SOM using the input vectors in a random order (i.e. not batch training)\n",
    "\n",
    "    # Plot distance map of the SOM\n",
    "\n",
    "    t = IRIS.target\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    plt.bone()\n",
    "    plt.pcolor(som.distance_map().T, cmap='Greys')\n",
    "    bar = plt.colorbar()\n",
    "    bar.set_label('Neuron Distance')\n",
    "    \n",
    "    # use different colors and markers for each label\n",
    "    markers = ['o', 's', '^']\n",
    "    colors = ['y', 'b', 'g']\n",
    "    labels = ['Iris Setosa', 'Iris Versicolor', 'Iris Virginica']\n",
    "    counter = [ False, False, False]\n",
    "    w_class = np.zeros([dimensions[0], dimensions[1], 3])\n",
    "    for cnt, xx in enumerate(inputData):\n",
    "        w = som.winner(xx)  # getting the winner\n",
    "        w_class[w[0], w[1], t[cnt]] +=1\n",
    "        \n",
    "        # place a marker on the winning position for the sample xx\n",
    "        if counter[t[cnt]] == False:\n",
    "            ax.plot(w[0]+.5, w[1]+.5, markers[t[cnt]], markerfacecolor='None',\n",
    "                     markeredgecolor=colors[t[cnt]], markersize=5, markeredgewidth=2, label=labels[t[cnt]])\n",
    "            counter[t[cnt]] = True\n",
    "        else:\n",
    "            ax.plot(w[0]+.5, w[1]+.5, markers[t[cnt]], markerfacecolor='None',\n",
    "                 markeredgecolor=colors[t[cnt]], markersize=5, markeredgewidth=2)\n",
    "    ax.axis([0, dimensions[0], 0, dimensions[1]])\n",
    "    ax.legend(loc=2, mode='expand', numpoints=1, ncol=4, fancybox = True)\n",
    "    plt.title(''.join([str(dimensions[0]), 'x' ,str(dimensions[1]),  ' SOM, after ' , str(steps) ,' training steps']))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    Class_acc = np.sum(np.amax(w_class, 2)) / np.sum(w_class)\n",
    "    print(\"classification Accuracy: \", Class_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_and_Plot_SOM(dimensions=[2, 2], inputData=[SepalLength, SepalWidth], steps=20)\n",
    "#InputData can be any combination of SepalLength, SepalWidth, PetalLength, PetalWidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Task 1.5:</b>  \n",
    "In your next task, please:  \n",
    "<b>1.</b> Comment on the quality of the above SOM - Does this provide any valuable insight? Why or why not? What do you need to change?  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.</b> In the python cell below, experiment on your own with some function calls to properly cluster the samples. What parameters lead to a result that looks 'well clustered' to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Task 1.6:</b>  \n",
    "Provide a few comments about how helpful you find the above SOM.  Did anything surprise you?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Task 1.7:</b>  \n",
    "Now, beginning with the SOM from <b>1.5.2</b> try to outperform the decision tree in the cell below. The decision tree is probably the simplest way to classify a dataset.  \n",
    "Your goal should be to just barely match the performance of the decision tree with as few parameters as possible!   \n",
    "Run the cell below to initialize your benchmark decision tree, then see how well SOM fares as a classifier, and finally comment on your resulting SOM.  Is it smaller or larger than the SOM from **1.5.2**, did that surprise you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Building a Decision Tree'\n",
    "\n",
    "Benchmark_tree = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "Benchmark_tree.fit(IRIS.data[:,2:], IRIS.target)\n",
    "print(\"This Decision tree's accuracy is: \", Benchmark_tree.score(IRIS.data[:,2:], IRIS.target))\n",
    "\n",
    "'Plotting the Decision boundaries'\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3]):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "    plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"Petal length\")\n",
    "    plt.ylabel(\"Petal width\")\n",
    "\n",
    "plot_decision_boundary(Benchmark_tree, IRIS.data[:,2:], IRIS.target)\n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
    "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
    "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
    "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
    "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the performance of the decision tree in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your answer in the cell below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 2: Self-organizing maps with Real-world Energy Data</h3>\n",
    "<br>\n",
    "In this exercise, you will be tasked to explore a provided dataset a little more independently. The dataset that you will be working with is energy consumption data of an industrial park. The park's owner has heard that you are taking an artificial intelligence course and asks you to use 'this machine learning stuff' to help with analyzing patterns in the park's energy consumption. Luckily, you remember that SOMs are considered 'machine learning stuff', and that they could actually be used for such a task.\n",
    "The dataset that the park provided looks like this:\n",
    "<ul>\n",
    "    <li>Energy consumed for each hour of the day in kWh (on the order of $10^{14}$)</li>\n",
    "    <li>Day of the week (0, 1,..., 6)</li>\n",
    "    <li>Month (1, 2,..., 12)</li>\n",
    "    <li>Holiday (1 = workday, 2 = holiday)</li>\n",
    "\n",
    "&emsp;<b>Task 2.1:</b>   \n",
    "Run the cells below. The first cell loads the dataset, does some slight preprocessing and packages it all in one array per day, separated into months. The dataset is then organized in the following way:\n",
    "February contains 28 arrays, for the 28 days in February.\n",
    "Each of those days, has in the 0th index the Weekday, the 1st index the Month, the 2nd index the type of day, and the rest is 24 power values for each hour of the day.  \n",
    "The second cell defines the EnergySOM function, and the third cell calls it for an exemplary dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "dataset = sio.loadmat('somdata.mat')\n",
    "dayInfo = dataset['dayInfo'].astype(float)\n",
    "rawHourlyEnergy = dataset['rawHourlyEnergy']\n",
    "\n",
    "\n",
    "# Scale the data to the interval [0, 1] for each dimension\n",
    "iScaler = preprocessing.MinMaxScaler()\n",
    "rawHourlyEnergy = iScaler.fit_transform(rawHourlyEnergy)\n",
    "dayInfo[:,0] = (1.0/np.amax(dayInfo[:,0]))*dayInfo[:,0]\n",
    "dayInfo[:,1] = (1.0/np.amax(dayInfo[:,1]))*dayInfo[:,1]\n",
    "dayInfo[:,2] = (1.0/np.amax(dayInfo[:,2]))*dayInfo[:,2]\n",
    "\n",
    "# Combine variables to obtain full input vector arrays\n",
    "inputData = np.hstack((dayInfo, rawHourlyEnergy))\n",
    "\n",
    "# Split the data into months\n",
    "january = inputData[0:31, :]\n",
    "february = inputData[31:59, :]\n",
    "march = inputData[59:90, :]\n",
    "april = inputData[90:120, :]\n",
    "may = inputData[120:151, :]\n",
    "june = inputData[151:181, :]\n",
    "july = inputData[181:212, :]\n",
    "august = inputData[212:243, :]\n",
    "september = inputData[243:273, :]\n",
    "october = inputData[273:304, :]\n",
    "november = inputData[304:334, :]\n",
    "december = inputData[334:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Energy Som Function'\n",
    "def EnergySOM(dimensions, InputCollection, steps, name):\n",
    "    dimensions = np.array(dimensions) # From list to np.array\n",
    "    \n",
    "    'Stacking the input data'\n",
    "    inputData = InputCollection[0]\n",
    "    if len(InputCollection) > 0:\n",
    "        for month in range(len(InputCollection)):\n",
    "            inputData = np.append(inputData, InputCollection[month], axis=0)\n",
    "    \n",
    "    'Create SOM'\n",
    "    som = MiniSom(\n",
    "        int(dimensions[0]),                                # Number of neurons (x-axis)\n",
    "        int(dimensions[1]),                                # Number of neurons (y-axis)\n",
    "        np.shape(inputData)[1],             # Number of elements in an input vector\n",
    "        sigma = .5,                        # Spread of the neighbourhood function\n",
    "        learning_rate = 0.5,                # Initial learning rate\n",
    "        neighborhood_function = 'gaussian',  # Type of neighborhood function\n",
    "        random_seed = 42 \n",
    "    )\n",
    "\n",
    "    'Train SOM'\n",
    "    som.random_weights_init(inputData)  # Initialize weights of neurons randomly\n",
    "    som.train_random(inputData, steps)  # Train the SOM using the input vectors in a random order (i.e. not batch training)\n",
    "    \n",
    "    'Plot the SOM using color plots'\n",
    "    plt.bone()  # Set color map\n",
    "    plt.pcolor(som.activation_response(inputData).T)  # Plot the number of hits on each neuron for a given set of input data\n",
    "    bar = plt.colorbar()\n",
    "    bar.set_label('Neuron Activation Level')\n",
    "    plt.axis([0, dimensions[0], 0, dimensions[1]])\n",
    "    plt.xlabel('Neuron Number')\n",
    "    plt.ylabel('Neuron Number')\n",
    "    plt.title(\"Neuron Activation Map: \" + \" \" + name)\n",
    "    plt.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnergySOM([3,4], [january[:, 1:20], february[:, 1:20]], 2000, 'Example 1')\n",
    "EnergySOM([3,4], [february[:, 1:25], march[:, 1:25]], 1870, 'Example 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Task 2.2:</b>  \n",
    "Ok, now it's your turn! You have to find some patterns in the data.\n",
    "This is purposely a more open ended question!\n",
    "Some ideas worth thinking about:  \n",
    "- Look for seasonal dependencies. Maybe January and December look similar, while June will look completely different?  \n",
    "- Maybe it is easier if you omit some data (like the month identifier?) to raise the sensitivity to other data?  \n",
    "- Possibly it makes sense to bundle similar months?  \n",
    "- To start it could be useful to plot all months for themselves at first?\n",
    "\n",
    "In the cell below, please provide your function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the insights you gained (if any) and what was your thought process behind your solution?  Remember we cannot read your thoughts, so everything you communicate that sounds like smart, logical engineering behavior is god input :-) (i.e. Deus Ex Machina).\n",
    "\n",
    "Additionally:  \n",
    "Be critical, what problems do you see with using SOM in such a task? What do you think the SOM learns to represent, etc..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Abstract</h2>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h3>Lab 4 Marking Guide</h3>\n",
    "<hr>\n",
    "</center>\n",
    "\n",
    "\\begin{array}{@{}clcc@{}}\n",
    "\\textbf{Exercise} & \\textbf{Item}           & \\textbf{Total Marks} & \\textbf{Earned Marks} \\\\ \n",
    "\\hline\n",
    "                  & Pre-lab                 & 10                     &               \\\\ \n",
    "                  & Abstract                & 3                     &               \\\\ \n",
    "                  & Introduction            & 3                     &               \\\\\n",
    "                  & Conclusion              & 4                     &               \\\\\n",
    "1                 & Clustering \t            & 63                     &               \\\\\n",
    "2                 & Pattern recognition     & 17                 \t&               \\\\\n",
    "\\hline\n",
    "                  & \\textbf{TOTAL}          & 100                    &\n",
    "\\end{array}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
