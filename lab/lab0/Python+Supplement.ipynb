{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ECE 449 - Intelligent Systems Engineering<br><br>\n",
    "Python Supplement</center></h1>\n",
    "<hr>\n",
    "\n",
    "<h2>1. Jupyter Notebook</h2>\n",
    "All labs will be completed in a **Jupyter notebook**, which is document that allows you to create and run Python code, while including visualizations and explanatory texts in the same document.  Consequently, the lab reports will also be completed in these documents.  The Jupyter notebook templates for each lab can be found on eClass.\n",
    "<br>\n",
    "To begin, go to https://cybera.syzygy.ca and log-in with your ualberta e-mail address.  Start your server, upload a Jupyter notebook file, and open it.  In order to run a cell, select it, and press SHIFT+ENTER.  Each cell has access to variables and functions in cells that were already run.  The **In []** to the left of a code cell indicates its current state:\n",
    "<br>\n",
    "<br>\n",
    "&emsp;**In [ ]** - the cell has not yet been run <br>\n",
    "&emsp;**In [&lowast;]** -the cell is currently being run <br>\n",
    "&emsp;**In [#]** - the cell has been run and in the order indicated by the number\n",
    "<br>\n",
    "<br>\n",
    "A cell has access to all of the variables and functions in previously run cells.  Therefore, **make sure that you run the cells with code that are initially provided in the template before proceeding with your own coding**.  Keyboard shortcuts can be found under HELP $\\rightarrow$ KEYBOARD SHORTCUTS.\n",
    "<br>\n",
    "When you have completed your notebook and are ready to submit it, convert the file to PDF by printing it to PDF.  This is accomplished by pressing CTRL + P or RIGHT CLICK $\\rightarrow$ PRINT.  Under the \"Destination\" option, change it to \"Save to PDF\", and then click Save.  Make sure that all of the necessary plots are displayed in the final document that you submit.  Document submission will be done through eClass, and each lab report is due a day before the next lab.\n",
    "<br>\n",
    "Various libraries are required for these labs.  Run the cell below to install these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# \"--user\" is essential to install in local environment\"\n",
    "pip install --user -U scikit-fuzzy\n",
    "pip install --user -U pyeasyga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Fuzzy Logic</h2>\n",
    "Assume that the following import statements are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np                       # General math operations\n",
    "import matplotlib.pyplot as plt          # Data visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D data visualization\n",
    "import skfuzzy as fuzz                   # Fuzzy toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 Defining a membership function</h3>\n",
    "A membership function can be defined in several ways.  A trapezoidal membership function requires four parameters: *a, b, c,* and *d*.  *a* and *d* specify the ends of the base, and *b* and *c* specify the ends of the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, num = 11)  # 0 to 10 with 11 elements\n",
    "Y = fuzz.trapmf(X, [1, 3, 6, 8])\n",
    "plt.plot(X, Y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title('Trapezoidal Membership Function [1, 3, 6, 8]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 Concentration and dilatation</h3>\n",
    "Concentration emphasizes points with higher membership values.  For example, concentration of \"small numbers\" turns into \"**very** small numbers\".  This is quantitatively represented by squaring the membership values:\n",
    "<br>\n",
    "<center>$Con(A) = A^2(x)$</center>\n",
    "<br>\n",
    "Dilatation of \"small numbers\" turns into \"**more or less** small numbers\", and can be quantitatively represented by taking the square root of the membership values:\n",
    "<br><br>\n",
    "<center>$Dil(A) = A^2(x)$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 20, num = 201)\n",
    "A = 1 / (1 + (X / 5) ** 3)  # \"small numbers\"\n",
    "verySmall = A ** 2\n",
    "moreOrLessSmall = A ** (1/2)\n",
    "verySmallHandle ,= plt.plot(X, verySmall, label = 'Very Small')\n",
    "smallHandle ,= plt.plot(X, A, label = 'Small')\n",
    "moreOrLessSmallHandle ,= plt.plot(X, moreOrLessSmall, label = 'More Or Less Small')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title('Concentration and Dilatation of Small Numbers')\n",
    "plt.legend(handles = [verySmallHandle, smallHandle, moreOrLessSmallHandle])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3 Union, intersection, and complement</h3>\n",
    "The union ($A \\cup B$), intersection ($A \\cap B$), and complement ($A^C$) are common operations performed on fuzzy sets.  There are various realizations of the union and intersection operations, but the most common forms are maximum and minimum, respectively.  The complement operation is always defined as $1 - A(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 20, num = 201)\n",
    "A = 1 / (1 + (X / 5) ** 3)  # \"small numbers\"\n",
    "B = 1 / (1 + 0.3 * (X - 8) ** 2)  # \"about 8\"\n",
    "[Y, union] = fuzz.fuzzy_or(X, A, X, B)  # Y, new universe of discourse, union, new MF\n",
    "[Z, intersection] = fuzz.fuzzy_and(X, A, X, B)\n",
    "complement = fuzz.fuzzy_not(A)\n",
    "\n",
    "smallHandle ,= plt.plot(X, A, label = 'Small Numbers')\n",
    "eightHandle ,= plt.plot(X, B, label = 'About 8')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title('Small Numbers and About 8 Fuzzy Sets')\n",
    "plt.legend(handles = [smallHandle, eightHandle])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X, union)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title('Union of A and B')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X, intersection)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title('Intersection of A and B')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X, complement)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title('Complement of A')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.4. $\\alpha$-cuts</h3>\n",
    "An $\\alpha$-cut is a crisp set that contains the elements that have membership greater than or equal to a certain value, $\\alpha$.  Consider a fuzzy set with membership function:\n",
    "<br>\n",
    "<center>$A(x) = \\frac{1}{1 + 0.01(x - 50)^2}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 100, num = 101)\n",
    "A = 1 / (1 + 0.01 * (X - 50) ** 2)\n",
    "alphaCut = (A >= 0.2)  # alpha-cut with alpha = 0.2\n",
    "AHandle ,= plt.plot(X, A, label = 'A')\n",
    "aCutHandle ,= plt.step(X, alphaCut, where = 'post', label = r'$\\alpha$ = 0.2')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title(r'$\\alpha$-cut of A with $\\alpha$ = 0.2')\n",
    "plt.legend(handles = [AHandle, aCutHandle])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.5 Relations</h3>\n",
    "Fuzzy relations between two fuzzy sets are performed using implication operators.  Common operators include Larsen implication (product) and Mamdani implication (minimum).  The relation can be expressed as:\n",
    "<br>\n",
    "<center>$R_{A \\rightarrow B}(x, y) = [A(x)B(y)]$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniA = np.array([0, 1, 2])\n",
    "uniB = np.array([0, 1, 2])\n",
    "A = np.array([0.3, 0.5, 0.2])\n",
    "B = np.array([0.2, 0.8, 0.4])\n",
    "larsen = fuzz.relation_product(A, B)  # Larsen implication\n",
    "print(\"larsen:\\n\", larsen)\n",
    "\n",
    "fig = plt.figure()\n",
    "[gX, gY] = np.meshgrid(uniA, uniB)\n",
    "ax = fig.gca(projection = '3d')\n",
    "ax.plot_surface(gX, gY, larsen)\n",
    "ax.set_xlabel('A')\n",
    "ax.set_ylabel('B')\n",
    "ax.set_zlabel('Membership')\n",
    "ax.set_title('A - B Relation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.6 Projections</h3>\n",
    "The x and y projections of $R(x, y)$ can be respectively expressed as:\n",
    "<br>\n",
    "<center>$ProjR_x(x) = sup_{y \\in Y}R(x, y)$ <br>\n",
    "$ProjR_y(y) = sup_{x \\in X}R(x, y)$</center>\n",
    "<br>\n",
    "The supremum ($sup$) operator often employed is the maximum operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xProj = np.amax(larsen, axis = 0, keepdims = True)\n",
    "yProj = np.amax(larsen, axis = 1, keepdims = True)\n",
    "print(\"xProj:\\n\", xProj)\n",
    "print(\"yProj:\\n\", yProj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.7 Reconstruction</h3>\n",
    "The reconstruction of a fuzzy relation is expressed as the Cartesian product of its projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction = yProj * xProj\n",
    "print(\"reconstruction:\\n\", reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.8 Cylindrical extension</h3>\n",
    "Cylindrical extension is defined as:\n",
    "<br>\n",
    "<center>$Cyl(A)(x, y) = A(x)\\ \\forall\\ y \\in Y$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cyl = np.concatenate([xProj, xProj, xProj])\n",
    "print(\"cyl:\\n\", cyl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.9 Sup-t composition</h3>\n",
    "Using relations $R_1$ and $R_2$, the sup-t composition can be performed with various **t** operations.  Some common combinations are max-min and max-product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R1 = np.array([[0.3, 0.5], [0.7, 0.2]])\n",
    "R2 = np.array([[0.4, 0.9], [0.8, 1.0]])\n",
    "R = fuzz.maxmin_composition(R1, R2)\n",
    "print(\"R:\\n\", R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.10 Compositional rule of inference</h3>\n",
    "Generalized modus ponens states that when a rule's antecedent is satisfied to some degree, its consequent can be inferred to the same degree.\n",
    "<br>\n",
    "<center>**IF** *x* **IS** A **THEN** *y* **IS** B<br>\n",
    "**IF** *x* **IS** A' **THEN** *y* **IS** B'</center>\n",
    "<br>\n",
    "This can be written using the implication relation ($R(x, y)$) as in the max-min composition:\n",
    "<br>\n",
    "<center>$B' = A' \\circ R(x, y)$</center>\n",
    "<br>\n",
    "Consider the rule \"**IF** *x* **IS** ABOUT 3 **THEN** *y* **IS** ABOUT 7\" with fuzzy sets FN3 = ABOUT 3 and FN7 = ABOUT 7 defined on the universe of discourse, X.  Applying a compositional rule of inference to a fuzzy singleton A = 4, using Larsen implication, yields the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, num = 11)\n",
    "FN3 = np.array([0, 0, 0.5, 1.0, 0.5, 0, 0, 0, 0, 0, 0])\n",
    "FN7 = np.array([0, 0, 0, 0, 0, 0.33, 0.67, 1.0, 0.67, 0.33, 0])\n",
    "larsen = fuzz.relation_product(FN3, FN7)\n",
    "\n",
    "# Define fuzzy singleton\n",
    "A = np.zeros(11)\n",
    "A[4] = 1\n",
    "\n",
    "fuzzySet = fuzz.maxprod_composition(A, larsen)\n",
    "\n",
    "plt.plot(X, fuzzySet.T)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Membership')\n",
    "plt.title('Consequent with Larsen Implication')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.11 Defuzzification</h3>\n",
    "There are various methods of determining the crisp value from a fuzzy set.  The most frequently used are the centroid and mean of maxima (MOM) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1, num = 6)\n",
    "fuzzySet = np.array([0, 0.165, 0.335, 0.5, 0.335, 0.165])\n",
    "crispValue = fuzz.defuzz(X, fuzzySet, 'mom')  # Mean of Maxima defuzzification method  (Use 'centroid' for the centroid method)\n",
    "print(\"crispValue = \", crispValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Neural Networks</h2>\n",
    "Assume that the following import statements are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np                               # General math operations\n",
    "import matplotlib.pyplot as plt                  # Data visualization\n",
    "from sklearn import preprocessing                # Data preprocessing\n",
    "from sklearn.linear_model import Perceptron      # Perceptron toolbox\n",
    "from sklearn.neural_network import MLPRegressor  # MLP toolbox\n",
    "from sklearn.neural_network import MLPClassifier # MLP toolbox\n",
    "from minisom import MiniSom                      # SOM toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.1 Preprocessing</h3>\n",
    "The Python library, *sklearn*, contains some common methods of data preprocessing.  This includes normalization, standard scaling, and minimum-maximum scaling.\n",
    "\n",
    "<h4>3.1.1 Normalization</h4>\n",
    "Using this function on an array of data normalizes each input vector in the array (along the x-axis) to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 1], [3, 4], [0.5, 0.5]])\n",
    "normX = preprocessing.normalize(X)\n",
    "print(\"normX:\\n\", normX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.1.2 Standard scaling</h4>\n",
    "This function standardizes features in the input data by removing the mean and scaling to unit variance.  When the input data is transformed, the scaler allows for the user to perform an inverse transform to revert the data back to its original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 1], [3, 4], [0.5, 0.5]])\n",
    "scaler = preprocessing.StandardScaler()\n",
    "stdX = scaler.fit_transform(X)\n",
    "invX = scaler.inverse_transform(stdX) # X = invX\n",
    "print(\"stdX:\\n\", stdX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.1.3 Minimum-maximum scaling</h4>\n",
    "Minimum-maximum scaling transforms the input data by scaling each feature to a given range, typically [0, 1].  Each column of the input array is scaled to this range, independently of the other columns.  Similar to standard scaling, an inverse transform function is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 1], [3, 4], [0.5, 0.5]])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "minmaxX = scaler.fit_transform(X)\n",
    "invX = scaler.inverse_transform(minmaxX)\n",
    "print(\"minmaxX:\\n\", minmaxX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2 Perceptron</h3>\n",
    "*sklearn* provides a way of building a simple perceptron, with the ability to set how many iterations of training it completes, and appropriate methods to employ the neural unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logical \"OR\" data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "Y = np.array([0, 1, 1, 1])  # Targets\n",
    "percep = Perceptron(n_iter = 50)\n",
    "percep.fit(X, Y)  # Train the perceptron\n",
    "Z = percep.predict(X)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.3 Multi-layer perceptron</h3>\n",
    "*MLPRegressor* and *MLPClassifier* in *sklearn* build a multi-layer perceptron, with the ability to customize various parameters, such as the number of units in each hidden layer, the activation function, and the initial learning rate.  It has similar training and prediction methods to the perceptron class.  Additionally, if the output values after each iteration are required, a *partial_fit* method is provided that trains the MLP for one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logical \"XOR\" data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "Y = np.array([0, 1, 1, 0])  # Targets\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes = (100,),  # One hidden layer of 100 units\n",
    "    activation = 'relu',\n",
    "    max_iter = 1000\n",
    ")\n",
    "mlp.fit(X, Y)  # Train the MLP\n",
    "Z = mlp.predict(X)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.4 Self-organizing map (Kohonen network)</h3>\n",
    "*MiniSom* is a Python library that implements self-organizing maps.  The constructor permits for customization of size, neighborhood function, and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constructor example\n",
    "som = MiniSom(\n",
    "    x,  # X dimension of SOM\n",
    "    y,  # Y dimension of SOM\n",
    "    input_len,  # Number of elements in an input vector\n",
    "    sigma = 1.0,  # Spread of neighborhood function\n",
    "    learning_rate = 0.5,  # Initial learning rate\n",
    "    neighborhood_function = 'gaussian'  # Neighborhood function type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the weights must be randomly initialized, and then the SOM needs to be trained.  The network can be trained using either a random method (*train_random*) or a batch method (*train_batch*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "som.random_weights_init(inputData)  # Initialize weights randomly\n",
    "som.train_random(inputData, noIter)  # Train randomly noIter times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the SOM results can be visualized using various methods, such as *distance_map*, which displays how close each neuron is to its neighboring neurons, and *activation_response*, which displays how many times each neuron was activated to an input array of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.pcolor(som.distance_map().T)\n",
    "plt.pcolor(som.activation_response(inputData).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Genetic Algorithms</h2>\n",
    "Assume that the following import statements are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyeasyga import pyeasyga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pyeasyga* is a Python library that allows for the creation and customization of genetic algorithms.  Only the input data needs to be specified when creating an algorithm, but numerous other parameters can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ga = pyeasyga.GeneticAlgorithm(\n",
    "    inputData,\n",
    "    population_size = 50,\n",
    "    generations = 100,\n",
    "    crossover_probability = 0.8,\n",
    "    mutation_probability = 0.2,\n",
    "    maximise_fitness = True  # Minimizes fitness if False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, functions to implement the individual creation, crossover, mutation, selection, and fitness functions need to be assigned to the GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_individual(data):\n",
    "    individual = ...\n",
    "    return individual\n",
    "\n",
    "def crossover(parent_1, parent_2):\n",
    "    child_1 = ...\n",
    "    child_2 = ...\n",
    "    return child_1, child_2\n",
    "\n",
    "def fitness(individual, data):\n",
    "    fitness = ...\n",
    "    return fitness\n",
    "\n",
    "def mutate(individual):\n",
    "    individual = ...\n",
    "\n",
    "ga.create_individual = create_individual\n",
    "ga.crossover_function = crossover\n",
    "ga.mutate_function = mutate\n",
    "ga.selection_function = ga.tournament_selection # pyeasyga's implemented selection function\n",
    "ga.fitness_function = fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the GA can be run, and the best solution can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ga.run()\n",
    "bestSoln = ga.best_individual()\n",
    "print(\"Fitness =\", bestSoln[0])\n",
    "print(\"Solution =\", bestSoln[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
